---
title: Bayesian halo
description: Modeling my chance of winning at Halo Infinite with bayesian stats
date: "`r Sys.Date()`"
author: Giuseppe Tinti Tomio
output:
    distill::distill_article:
        toc: true
repository_url: https://github.com/GiuseppeTT/bayesian-halo
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
    echo = FALSE,
    warning = FALSE,
    message = FALSE
)
```

```{r libraries}
library(tidyverse)
library(gt)
library(targets)
```

```{r sources}
source("R/constants.R")
```

# Too long, didn't read

**Introduction:** I used bayesian stats to model a match of [Halo Infinite](https://www.xbox.com/games/halo-infinite). The game consists of 2 teams of 12 players that must score points by defeating players of the other team. A team wins if it is the first to achieve 100 points or has the biggest score by the 15 minutes mark.

**Results:** The model obtained a median absolute error of `r scales::number(60 * tar_read(test_model_mae), accuracy = 10^(-DECIMALS))` seconds in predicting when each team will score its next point. Moreover, it provided powerful insights such as estimates for the teams' performance and the probability of a team winning at any given time.

**Conclusion:** The model was little better than a cumulative average (median absolute error of `r scales::number(60 * tar_read(test_base_mae), accuracy = 10^(-DECIMALS))` seconds). Therefore, the bayesian model is well suited for a highly accurate analysis, but fails to justify its complexity and computational demand for simpler applications.

# Introduction

If you are like me, you have been playing [Halo Infinite](https://www.xbox.com/games/halo-infinite) a lot in the past weeks. One game mode that I enjoy is Big Team Battle Slayer (BTBS) in which 2 teams of 12 players must score points by defeating players of the other team. A team wins if it is the first to achieve 100 points or has the biggest score by the 15 minutes mark.

Now, if you are really like me, you have also been wondering about how to calculate the chance of winning. Intuitively, I know that if the score is 20 - 10 for my team I have a slight advantage, whereas 90 - 80 is a certain win. But how can I quantify this probability? This is the question that my work sets out to answer using bayesian statistics.

# Dataset

To answer the proposed question, I selected two matches of Halo BTBS from YouTube and annotated the game statistics. The [first game](https://youtu.be/cANMWiYTD84) defines the train dataset. It was used to better understand this type of data and propose a model. The [second game](https://youtu.be/kuH9nhdzt64) defines the test dataset. It was used to fit the proposed model in a new dataset and check the model's performance.

Each dataset is composed of three variables (columns):

- **time:** Time at which the score was recorded.
- **blue:** Score of team blue at the time. Team blue is the team of the player who recorded the gameplay.
- **red:** Score of team red at the time. Team red is the enemy team of the player who recorded the gameplay.

Due to the way the data was recorded, there are a few concerns which could not be addressed:

- The data might not be representative because it was extracted from YouTube. This is due to the fact that people tend to publish only games in which they had a good performance.
- The data was manually extracted, which means that there may be a difference between the recorded and the actual time for each score point.
- The maximum time resolution is 1 second as it is constrained by the in-game time resolution, obtained from the scoreboard timer.

<!--
    Besides, there are times in which the scoreboard is not visible or is only partially visible which might have impacted the annotated data.
-->

All things considered, that data should be enough for developing a proof of concept.

# Exploratory data analysis

In the exploratory analysis, I demonstrate how the following properties about the data hold true, at least to an approximate extent.

- **Independent points** : Each point is earned independently from the other points.
- **Constant average** : The average time between points is constant throughout the game. The average differs between the teams.
- **Exponentially distributed** : The time between points follows an exponential distribution. The distribution differs between the teams.

We begin the analysis with Figure \@ref(fig:train-observed-score-plot) which summarizes the train dataset. It shows the score progression for both teams. Besides the clear win for team blue, a pattern stands out. The score trajectories are linear, which suggests that the teams earn points at a constant rate.

```{r train-observed-score-plot, fig.cap = "Score progression for both teams. As observed in the train dataset."}
tar_read(train_observed_score_plot)
```

To better explore the point rate, we refer to Figure \@ref(fig:train-observed-tbp-vs-score-plot). It shows the time between points (TBP) for each team as a function of score. As it can be seen from the regression curves, the average TBP is constant throughout the game.

<!--
Perhaps the only exception is the first score point which took roughly 30 seconds for both teams. This is due to the fact that the players start at their bases and must walk to the middle of the battlefield at the start of the game.
-->

```{r train-observed-tbp-vs-score-plot, fig.cap = "TBP for each team as a function of score. As observed in the train dataset.", preview = TRUE}
tar_read(train_observed_tbp_vs_score_plot)
```

Still about the time between points, Figure \@ref(fig:train-observed-tbp-plot) plots the estimated TBP distribution for each team. The density curves have an exponential decay which favors an exponential distribution.

```{r train-observed-tbp-plot, fig.cap = "Estimated TBP distribution for each team. As observed in the train dataset."}
tar_read(train_observed_tbp_plot)
```

To evaluate the independence between points, Figure \@ref(fig:train-observed-tbp-vs-lag-tbp-plot) shows TBP vs previous TBP for each team. Taking into account the estimation error, the regression curves are reasonably constant. This means that there is no dependence between points within each team.

```{r train-observed-tbp-vs-lag-tbp-plot, fig.cap = "TBP for each team as a function of previous TBP. As observed in the train dataset."}
tar_read(train_observed_tbp_vs_lag_tbp_plot)
```

Finally, we take a look at the correlation between teams. Figure \@ref(fig:train-observed-window-mean-tbp) exhibits team blue's TBP as a function of team red's TBP, averaged over periods of `r WINDOW_SIZE` seconds. The horizontal regression curve indicates that team red has no influence on team blue and vice versa.

```{r train-observed-window-mean-tbp, fig.cap = str_glue("Team blue's TBP as a function of team red's TBP, averaged over periods of {WINDOW_SIZE} seconds. As observed in the train dataset.")}
tar_read(train_observed_window_tbp_plot)
```

# Model

Accounting for all the discoveries made in the exploratory data analysis, I came up with the following bayesian model.

```python
# Part 1: Prior hyperparameter
MAX_SCORE = 100
PRIOR_MEAN_TIME = 10
mean_rate_of_rates = MAX_SCORE / PRIOR_MEAN_TIME
rate_of_rates = 1 / mean_rate_of_rates

# Part 2: Parameters
blue_point_rate ~ Exponential(rate_of_rates)
red_point_rate ~ Exponential(rate_of_rates)

# Part 3: Likelihood
blue_tbp ~ Exponential(blue_point_rate)
red_tbp ~ Exponential(red_point_rate)
```

<!-- The second and third sentences could be simplified so that the paragraph takes only 3 lines -->
Part 1 summarizes my knowledge before the game even starts. My prior is that teams take on average 10 minutes to get 100 points.

```R
# Part 1: Prior hyperparameter
MAX_SCORE = 100
PRIOR_MEAN_TIME = 10
mean_rate_of_rates = MAX_SCORE / PRIOR_MEAN_TIME
rate_of_rates = 1 / mean_rate_of_rates
```

Part 2 reports how parameters are sampled from the prior. Here, `red_point_rate` is team red's point rate and measures how many points team red makes per minute on average.

```R
# Part 2: Parameters
blue_point_rate ~ Exponential(rate_of_rates)
red_point_rate ~ Exponential(rate_of_rates)
```

Lastly, part 3 states that TBP follows an exponential distribution with the specified point rates. Moreover, each point is assumed to be independent from the others.

```R
# Part 3: Likelihood
blue_tbp ~ Exponential(blue_point_rate)
red_tbp ~ Exponential(red_point_rate)
```

# Prior model analysis

Before fitting the model to the data, it is interesting to check if the selected prior is appropriate. To do that, we can simulate data from the model using only the prior information.

Figure \@ref(fig:prior-score-plot) summarizes the simulation. It shows possible score progressions from a hypothetical team. As it can be seen, the model allows for a wide range of score progressions.

```{r prior-score-plot, fig.cap = "Score progression for a hypothetical team. As simulated from the prior model."}
tar_read(prior_model_score_plot)
```

# Results

## Model performance

To avoid overfitting, we now turn to the test dataset to check the model's performance. Figure \@ref(fig:test-observed-score-plot) shows the score progression for both teams as observed in the test dataset. Notice that the test dataset is a close match and is very different from the train dataset.

```{r test-observed-score-plot, fig.cap = "Score progression for both teams. As observed in the test dataset."}
tar_read(test_observed_score_plot)
```

The model attained a good performance in the test dataset and Figure \@ref(fig:test-model-prediction-plot) makes it very clear. It shows the observed (black dot) and predicted (gray line) time for each score point. The fact that it is difficult to distinguish one from the other is a good sign.

```{r test-model-prediction-plot, fig.cap = "Observed (black dot) and predicted (blue line) time for each score point. As observed in the test dataset."}
tar_read(test_model_prediction_plot)
```

Figure \@ref(fig:test-model-residue-plot) allows us to zoom in into the model errors. It plots the error (black line) made for each score point. The errors are quite small, with a median size of `r scales::number(60 * tar_read(test_model_mae), accuracy = 10^(-DECIMALS))` seconds.

Figure \@ref(fig:test-model-residue-plot) also shows shaded regions. They should contain most of the error line and represent how wrong the model thinks it can be. Since the regions cover `r scales::percent(tar_read(test_model_coverage), accuracy = 10^(-DECIMALS))` of the errors and are narrow (median size of `r scales::number(60 * tar_read(test_model_interval_median_size), accuracy = 10^(-DECIMALS))` seconds), the model confidence is on point.

```{r test-model-residue-plot, fig.cap = "Error (black line) made for each score point. Shaded regions should contain most of the error line and represent how wrong the model thinks it can be. As observed in the test dataset."}
tar_read(test_model_residue_plot)
```

## Inference

Now that we know that the model is well fit, we can turn to it for answers on how good the teams are. Table \@ref(tab:test-model-rate-table) summarizes the point rates. Despite team red winning (estimated point rate of `r tar_read(test_model_rate_table) %>% filter(rate == "Red") %>% pull(.value) %>% scales::number(accuracy = 10^(-DECIMALS))` points per minute), the difference between point rates (contrast) is not significant. This means that for all purpose and intent, both teams are equally good.

```{r test-model-rate-table}
tar_read(test_model_rate_table) %>%
    select(
        rate,
        .value,
        .lower,
        .upper
    ) %>%
    gt(caption = "Posterior estimate results for the test data.") %>%
    cols_align("left", where(is.factor)) %>%
    fmt_number(where(is.numeric), decimals = DECIMALS) %>%
    cols_merge(c(.lower, .upper),pattern = "[{1}, {2}]") %>%
    cols_label(
        rate = "Point rate",
        .value = "Median",
        .lower = "95% Credible interval"
    )
```

## Probability of winning

Finally, Figure \@ref(fig:test-model-winning-probabilities-plot) displays team blue's probability of winning (black line) as predicted by the model. It suggests an advantage for team blue during most of the game. This agrees with Figure \@ref(fig:test-observed-score-plot) in which team blue has more points for most of the game.

Again, the shaded region summarizes other possible probabilities and represents how wrong the model thinks it can be. An interesting part is the beginning of the match when team blue had 2.5 times the number of points of team red. As per the model, team blue should have won with a great confidence, but team red was able to turn the game and win at the last minute.

```{r test-model-winning-probabilities-plot, fig.cap = "Team blue's probability of winning (black line) as predicted by the model for each time point. Shaded region summarizes other possible probabilities and represents how wrong the model thinks it can be. As observed in the test dataset."}
tar_read(test_model_winning_probabilities_plot)
```

## Baseline comparison

For the last remark, we need to compare how the model performs compared to a baseline. Figure \@ref(fig:test-base-prediction-plot) summarizes the predictions made by a cumulative average. That is, the next point for team blue is predicted to happen at `current_time` `+` `blue_average_tbp`. Just like for the bayesian model, it is difficult to distinguish the predictions from the observed data.

```{r test-base-prediction-plot, fig.cap = "Observed (black dot) and predicted (blue line) time for each score point. As observed in the test dataset with the cumulative average model."}
tar_read(test_base_prediction_plot)
```

Figure \@ref(fig:test-base-residue-plot) plots the error made for each score point. The median absolute error is `r scales::number(60 * tar_read(test_base_mae), accuracy = 10^(-DECIMALS))` seconds and the intervals have a median size of `r scales::number(60 * tar_read(test_base_interval_median_size), accuracy = 10^(-DECIMALS))` seconds (coverage = `r scales::percent(tar_read(test_base_coverage), accuracy = 10^(-DECIMALS))`). In summary, the bayesian model obtained results about 20% better than the baseline.

```{r test-base-residue-plot, fig.cap = "Error (black line) made for each score point. Shaded regions should contain at least 95% of the errors. As observed in the test dataset with the cumulative average model."}
tar_read(test_base_residue_plot)
```

# Conclusion

The model showed a good fit and was able to provide powerful insights such as the probability of team blue winning at any time point. That being said, the performance was little better than a cumulative average. Therefore, the bayesian model is well suited for a highly accurate analysis, but fails to justify its complexity and computational demand for simpler applications.
