---
title: Bayesian halo
description: TODO
date: "`r Sys.Date()`"
author: Giuseppe Tinti Tomio
output:
    distill::distill_article:
        toc: true
repository_url: https://github.com/GiuseppeTT/bayesian-halo
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
    echo = FALSE,
    warning = FALSE,
    message = FALSE
)
```

```{r libraries}
library(tidyverse)
library(gt)
library(targets)
```

```{r sources}
source("R/constants.R")
```

# Too long, didn't read

I used bayesian stats to model a match of Big Team Battle Slayer (BTBS). BTBS is a game mode of Halo Infinite in which 2 teams score points by defeating players of the other team. A team wins if it is the first to achieve 100 points or has the biggest score by the 15 minutes mark.

The model performed better than a linear regression at predicting the game progression. Moreover, the model can be easily used to estimate the probability of a team winning at each timepoint.

# Introduction

If you are like me, you have been playing [Halo Infinite](https://www.xbox.com/games/halo-infinite) a lot in the past weeks. One game mode that I particularly like is Big Team Battle Slayer (BTBS) in which 2 teams of 16 players face each other on a battlefield. Each time one player of team blue defeats a player of team red, team blue earns a point. The first team to achieve 100 points wins the game. If no team hits 100 points by the 15 minutes mark, the team with the biggest score wins.

Now if you are really like me, you have also been wondering about how to statistically model the game. Specifically, I was interested in calculating the probability of my team winning given the current score. Intuitively, I know that there is a 50%-50% chance of winning when the game starts. If the score is 20 - 10 for my team, I also expect to have a higher although not extreme probability of winning. But what about when the score is 90 - 80? From my experience this game is a certain win, but how should I quantify this probability? This is the question that my work sets out to answer using bayesian statistics.

<!--
    The report is divided into sections. Section [Dataset] describes the dataset used and Section [Exploratory data analysis] presents an exploratory analysis of its content to understand statistical properties of the game data. Based on this exploratory analysis, Section [Model] defines a Bayesian model for the data and Section [Prior model analysis] shows a synthetic dataset for the given prior values. Finally, Section [Results] shows the results obtained when fitting the model to the full data and [Conclusion] finishes the report with the final conclusions.
-->

# Dataset

To answer the question proposed, I selected two matches of Halo BTBS from YouTube and manually filled two spreadsheets with game statistics. The first game is available at https://youtu.be/cANMWiYTD84 and defines the train dataset. It was used to gain some quantitative knowledge about this type of data, propose a model and check valid values for a prior distribution. The second game is available at https://youtu.be/kuH9nhdzt64 and defines the test dataset. It was used to fit the proposed model in a novel dataset to check the model's performance.

Each dataset is composed of three variables (columns). They are:

- **time:** Time at which the score was recorded.
- **blue:** Score of team blue at the time. Team blue is the team of the player who recorded the gameplay.
- **red:** Score of team red at the time. Team red is the enemy team of the player who recorded the gameplay.

Due to the way the data was recorded, there are a few concerns which could not be addressed. They are:

- The data was extracted from a gameplay available at YouTube and, therefore, might not be representative. This is because people tend to publish only games where they had a good performance.
- The data was manually extracted, which means that there may be a difference between the recorded and the actual time of each score point. Besides, there are times in which the scoreboard is not visible or is only partially visible which might have impacted the annotated data.
- The maximum time resolution is 1 second as it is constrained by the in-game time resolution, obtained from the scoreboard timer.

All things considered, that data should be enough for developing a proof of concept.

# Exploratory data analysis

In the exploratory analysis, I demonstrate how the following properties about the data hold true, at least to an approximate extent.

- **Independent points** : Each point is earned independently from the other points.
- **Constant average** : The average time between points is constant throughout the game. The average differs between the teams.
- **Exponentially distributed** : The time between points follows an exponential distribution. The distribution differs between the teams.

We begin the analysis with Figure \@ref(fig:train-observed-score-plot) which summarizes the data. It shows the score progression for both teams as observed in the train dataset. Besides the clear win for team blue, a pattern stands out. The score trajectories are linear, which suggests that the teams earn points at a constant rate.

```{r train-observed-score-plot, fig.cap = "Score progression for both teams. As observed in the train dataset."}
tar_read(observed_score_plot)
```

To better explore the point rate, we refer to Figure \@ref(fig:train-observed-tbp-vs-score-plot). It shows the time between points (TBP) for each team as a function of score. As it can be seen by the regression curves, the average TBP is constant throughout the game. This fact confirms the constant point rate.

<!--
Perhaps the only exception is the first score point which took roughly 30 seconds for both teams. This is due to the fact that the players start at their bases and must walk to the middle of the battlefield at the start of the game.
-->

```{r train-observed-tbp-vs-score-plot, fig.cap = "Time between points (TBP) for each team as a function of score. As observed in the train dataset."}
tar_read(observed_tbp_vs_score_plot)
```

Still about the time between points, Figure \@ref(fig:train-observed-tbp-plot) plots the estimated distribution for each team. The estimated distributions have an exponential decay which favors an exponential distribution.

```{r train-observed-tbp-plot, fig.cap = "Estimated TBP distribution for each team. As observed in the train dataset."}
tar_read(observed_tbp_plot)
```

To evaluate the independence between points, Figure \@ref(fig:train-observed-tbp-vs-lag-tbp-plot) shows TBP vs previous TBP for each team. Taking into account the estimation error, the regression curves are reasonably constant. This means that there is no dependence between points in each team.

```{r train-observed-tbp-vs-lag-tbp-plot, fig.cap = "Time between points (TBP) for each team as a function of lag TBP. As observed in the train dataset."}
tar_read(observed_tbp_vs_lag_tbp_plot)
```

```{r train-permuted-tbp-vs-lag-tbp-plot, fig.cap = "Time between points (TBP) for each team as a function of lag TBP. As observed in the train dataset with shuffled TBP."}
# tar_read(permuted_tbp_vs_lag_tbp_plot)
```

Finally, we take a look at the relation between the teams to inspect for any correlation between them. Figure \@ref(fig:train-observed-window-mean-tbp) shows the average TBP for every `r WINDOW_SIZE` seconds between team blue and red. The horizontal regression line evidentiates that team red has no influence at team blue's point rate and vice versa.

```{r train-observed-window-mean-tbp, fig.cap = "Mean time between points (TBP) for team blue vs team red. Windows size is 15 seconds. As observed in the train dataset."}
tar_read(window_mean_tbp_plot)
```

# Model

Accounting for all the discoveries made in the exploratory data analysis, I came up with the following bayesian model.

```R
# Part 1: Data
point_time[blue][1], ..., point_time[blue][blue_last_score]
point_time[red][1], ..., point_time[red][red_last_score]

# Part 2: Transformed data
tbp[blue][i] = point_time[blue][i] - point_time[blue][i - 1]
tbp[red][j] = point_time[red][j] - point_time[red][j - 1]

# Part 3: Prior
MAX_SCORE = 100
PRIOR_MEAN_TIME = 10
mean_rate_of_rates = MAX_SCORE / PRIOR_MEAN_TIME
rate_of_rates = 1 / mean_rate_of_rates

# Part 4: Parameters
point_rate[blue] ~ Exponential(rate_of_rates)
point_rate[red] ~ Exponential(rate_of_rates)

# Part 5: Likelihood
tbp[blue][i] ~ Exponential(point_rate[blue])
tbp[red][j] ~ Exponential(point_rate[red])
```

Part 1 describes the input data, which is the time at which the teams scored each point. For instance, `point_time[blue][5]` is the time point in which the blue team scored its 5th point.

```R
# Part 1: Data
point_time[blue][1], ..., point_time[blue][blue_last_score]
point_time[red][1], ..., point_time[red][red_last_score]
```

Part 2 shows how to calculate TBP from the given data. For example, `tbp[red][4]` is the amount of time that elapsed between the 3rd and 4th point of team red. It is obtained by simply subtracting `point_time[red][3]` from `point_time[red][4]`.

```R
# Part 2: Transformed data
tbp[blue][i] = point_time[blue][i] - point_time[blue][i - 1]
tbp[red][j] = point_time[red][j] - point_time[red][j - 1]
```

Part 3 summarizes my knowledge before the game even starts. My prior is that teams take on average 10 minutes to get 100 points. On top of that, using an exponential distribution as a prior guarantees that the final model is not too restricted. For instance, the selected prior allows for teams to take from `r round(100 / qexp(1 - 0.05 / 2, rate = 1 / PRIOR_MEAN_RATE_OF_RATES))` to `r round(100 / qexp(0.05 / 2, rate = 1 / PRIOR_MEAN_RATE_OF_RATES))` minutes to achieve 100 points.

```R
# Part 3: Prior
MAX_SCORE = 100
PRIOR_MEAN_TIME = 10
mean_rate_of_rates = MAX_SCORE / PRIOR_MEAN_TIME
rate_of_rates = 1 / mean_rate_of_rates
```

Part 4 explains how the parameters are sampled from the prior. Here, `point_rate[blue]` is the point rate from the blue team and measures how many points team blue makes per minute on average.

```R
# Part 4: Parameters
point_rate[blue] ~ Exponential(rate_of_rates)
point_rate[red] ~ Exponential(rate_of_rates)
```

Lastly part 5 states that TBP follows an exponential distribution with the specified parameters. More importantly, each point is assumed to be independent from the others.

```R
# Part 5: Likelihood
tbp[blue][i] ~ Exponential(point_rate[blue])
tbp[red][j] ~ Exponential(point_rate[red])
```

# Prior model analysis

Before fitting the model to the data, it is interesting to check if the selected prior is appropriate. To do that, we can simulate data from the model using only the prior information. That is, without providing data to the model. Figure \@ref(fig:prior-score-plot) summarizes the simulated data. It shows the score progression for a hypothetical team as simulated from the prior model.

As it can be seen, the models allow for a wide range of score progressions. The only restriction is that the trajectories should be linear.

```{r}
# tar_read(prior_model_rate_plot)
```

```{r prior-score-plot, fig.cap = "Score progression for a hypothetical team. As simulated from the prior model."}
tar_read(prior_model_score_plot)
```

```{r}
# tar_read(prior_model_tbp_vs_score_plot)
```

```{r}
# tar_read(prior_model_tbp_plot)
```

# Results

## Train dataset

```{r}
tar_read(train_model_rate_table) %>%
    select(
        rate,
        .value,
        .lower,
        .upper
    ) %>%
    gt(caption = "Posterior estimate results for the train data.") %>%
    cols_align("left", where(is.factor)) %>%
    fmt_number(where(is.numeric), decimals = DECIMALS) %>%
    cols_merge(c(.lower, .upper),pattern = "[{1}, {2}]") %>%
    cols_label(
        rate = "Point rate",
        .value = "Median",
        .lower = "95% HDCI"
    )
```

```{r}
#tar_read(train_model_rate_plot)
```

```{r}
tar_read(train_model_score_plot)
```

```{r}
# tar_read(train_model_tbp_vs_score_plot)
```

```{r}
# tar_read(train_model_tbp_plot)
```

## Test dataset

```{r}
tar_read(test_model_rate_table) %>%
    select(
        rate,
        .value,
        .lower,
        .upper
    ) %>%
    gt(caption = "Posterior estimate results for the test data.") %>%
    cols_align("left", where(is.factor)) %>%
    fmt_number(where(is.numeric), decimals = DECIMALS) %>%
    cols_merge(c(.lower, .upper),pattern = "[{1}, {2}]") %>%
    cols_label(
        rate = "Point rate",
        .value = "Median",
        .lower = "95% HDCI"
    )
```

```{r}
# tar_read(test_model_rate_plot)
```

```{r}
tar_read(test_model_score_plot)
```

```{r}
# tar_read(test_model_tbp_vs_score_plot)
```

```{r}
# tar_read(test_model_tbp_plot)
```

# Conclusion

TODO
